{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''As per the defination provided on pytorch : https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html\n",
    "I have created a Custom batch sample following the same implementation on above link.\n",
    "Below implementation of sampler is taken from forum https://discuss.pytorch.org/t/load-the-same-number-of-data-per-class/65198/3'''\n",
    "class CustomBatchSampler(BatchSampler):\n",
    "\n",
    "    def __init__(self, dataset, n_classes, n_samples):\n",
    "        loader = DataLoader(dataset)\n",
    "        self.labels_list = []\n",
    "        for _, label in loader:\n",
    "            self.labels_list.append(label)\n",
    "        self.labels = torch.LongTensor(self.labels_list)\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < len(self.dataset):\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "#load training data set.\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "#Load testing data set.\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " {'T-shirt/top': 6000, 'Trouser': 6000, 'Pullover': 6000, 'Dress': 6000, 'Coat': 6000, 'Sandal': 6000, 'Shirt': 6000, 'Sneaker': 6000, 'Bag': 6000, 'Ankle boot': 6000}\n"
     ]
    }
   ],
   "source": [
    "'''Checking the data set classes and amount of data in each class.'''\n",
    "idx2class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n",
    "    \n",
    "    for element in dataset_obj:\n",
    "        y_lbl = element[1]\n",
    "        y_lbl = idx2class[y_lbl]\n",
    "        count_dict[y_lbl] += 1\n",
    "            \n",
    "    return count_dict\n",
    "print(\"Distribution of classes: \\n\", get_class_distribution(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "total_classes = 10\n",
    "epochs_count = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For different configuration just uncomment below line and execute all cells.'''\n",
    "batch_sampler = CustomBatchSampler(train_dataset, total_classes, 100)\n",
    "#batch_sampler = CustomBatchSampler(train_dataset, total_classes, 500)\n",
    "#batch_sampler = CustomBatchSampler(train_dataset, total_classes, 1000)\n",
    "#batch_sampler = CustomBatchSampler(train_dataset, total_classes, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "'''For running on the full dataset, uncomment this line and comment above line.'''\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with 2 layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, total_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_images_per_class = 1000\n",
    "        self.n_classes = 10\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size) \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.l2 = torch.nn.Linear(hidden_size, total_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = NeuralNet(input_size, hidden_size, total_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cross Entropy Loss.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "#Using Adam optimizer\n",
    "optimizer = torch.optim.Adam(nnmodel.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the NN\n",
    "steps = len(train_loader)\n",
    "for epoch in range(epochs_count):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = nnmodel(images)\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{epochs_count}], Step [{i+1}/{steps}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images: 83.52 %\n"
     ]
    }
   ],
   "source": [
    "# Test the NN\n",
    "with torch.no_grad():\n",
    "    nn_correct = 0\n",
    "    nn_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = nnmodel(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        nn_samples += labels.size(0)\n",
    "        nn_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * nn_correct / nn_samples\n",
    "    print(f'Accuracy of the network on test images: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72        11\n",
      "           1       0.92      0.92      0.92        12\n",
      "           2       0.33      0.50      0.40         6\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       0.86      0.75      0.80         8\n",
      "           5       1.00      0.82      0.90        11\n",
      "           6       0.60      0.25      0.35        12\n",
      "           7       0.88      1.00      0.93         7\n",
      "           8       0.88      1.00      0.93        14\n",
      "           9       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.80      0.81      0.79       100\n",
      "weighted avg       0.82      0.81      0.80       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(labels, predicted, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  0  0  0  0  0  1  0  1  0]\n",
      " [ 0 11  1  0  0  0  0  0  0  0]\n",
      " [ 1  1  3  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  6  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  1]\n",
      " [ 4  0  4  0  1  0  3  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(confusion_matrix(labels,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
